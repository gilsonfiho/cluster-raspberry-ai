import ollama
import time
import yaml


def list_viable_models(memory_threshold_mb):
    response = ollama.list()

    print(f"DEBUG MODELS RESPONSE: {response}")  # ğŸ” Debug opcional

    model_names = []

    # Se a resposta for uma lista de tuplas
    if isinstance(response, list) and isinstance(response[0], tuple):
        model_names = [item[0] for item in response]

    # Se vier como dicionÃ¡rio (modelos recentes da API)
    elif isinstance(response, dict) and 'models' in response:
        model_names = [m.get("name") or m.get("model") for m in response['models']]

    # Se vier como lista de dicionÃ¡rios (outra variante)
    elif isinstance(response, list) and isinstance(response[0], dict):
        model_names = [m.get("name") or m.get("model") for m in response]

    else:
        print("âŒ Formato desconhecido da resposta do Ollama:", response)
        exit(1)

    return model_names


def run_benchmark(model, prompt):
    start_time = time.time()

    try:
        response = ollama.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        end_time = time.time()

        output = response['message']['content']
        elapsed = end_time - start_time

        return {
            "model": model,
            "output": output,
            "time_seconds": round(elapsed, 2)
        }

    except Exception as e:
        print(f"âŒ Erro ao rodar {model}: {e}")
        return {
            "model": model,
            "output": "Erro",
            "time_seconds": None
        }


def main():
    # ğŸ”§ Carregar configuraÃ§Ãµes
    with open('config.yaml', 'r') as file:
        config = yaml.safe_load(file)

    memory_limit = config.get("memory_threshold_mb", 3500)
    prompt = config.get("prompt", "Explique brevemente o que Ã© aprendizado de mÃ¡quina?")

    print(f"ğŸš€ Benchmark com limite de memÃ³ria {memory_limit} MB")
    print(f"ğŸ§  Prompt usado: {prompt}")

    # ğŸ” Listar modelos possÃ­veis
    models = list_viable_models(memory_limit)
    print(f"âœ… Modelos viÃ¡veis: {models}")

    if not models:
        print("âš ï¸ Nenhum modelo disponÃ­vel dentro do limite de memÃ³ria.")
        return

    results = []

    for model in models:
        print(f"â³ Testando {model} ...")
        result = run_benchmark(model, prompt)
        results.append(result)
        print(f"âœ”ï¸ {model} respondeu em {result['time_seconds']}s")

    # ğŸ“œ Gerar relatÃ³rio
    print("\nğŸ“Š Resultado Final:")
    print("-" * 50)
    for r in results:
        print(f"ğŸ§  {r['model']}")
        print(f"â±ï¸ Tempo: {r['time_seconds']}s")
        print(f"â¡ï¸ Resposta: {r['output']}")
        print("-" * 50)


if __name__ == "__main__":
    main()
