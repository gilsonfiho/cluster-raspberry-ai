import ollama
import time
import yaml


def list_viable_models(memory_threshold_mb):
    response = ollama.list()

    if isinstance(response, dict) and 'models' in response:
        models = response['models']
    else:
        models = response

    if not models:
        print("‚ùå Nenhum modelo encontrado. Verifique se o Ollama est√° rodando e se h√° modelos instalados.")
        exit(1)

    model_names = []
    for m in models:
        name = m.get("name") or m.get("model")
        size = m.get("size", 0) / (1024 * 1024)  # Bytes ‚Üí MB
        if name and size <= memory_threshold_mb:
            model_names.append(name)

    return model_names


def run_benchmark(model, prompt):
    start_time = time.time()

    try:
        response = ollama.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        end_time = time.time()

        output = response['message']['content']
        elapsed = end_time - start_time

        return {
            "model": model,
            "output": output,
            "time_seconds": round(elapsed, 2)
        }

    except Exception as e:
        print(f"‚ùå Erro ao rodar {model}: {e}")
        return {
            "model": model,
            "output": "Erro",
            "time_seconds": None
        }


def main():
    # üîß Carregar configura√ß√µes
    with open('config.yaml', 'r') as file:
        config = yaml.safe_load(file)

    memory_limit = config.get("memory_threshold_mb", 2500)
    prompt = config.get("prompt", "Qual √© a capital da Fran√ßa?")

    print(f"üöÄ Benchmark com limite de mem√≥ria {memory_limit} MB")
    print(f"üß† Prompt usado: {prompt}")

    # üîç Listar modelos poss√≠veis
    models = list_viable_models(memory_limit)
    print(f"‚úÖ Modelos vi√°veis: {models}")

    if not models:
        print("‚ö†Ô∏è Nenhum modelo dispon√≠vel dentro do limite de mem√≥ria.")
        return

    results = []

    for model in models:
        print(f"‚è≥ Testando {model} ...")
        result = run_benchmark(model, prompt)
        results.append(result)
        print(f"‚úîÔ∏è {model} respondeu em {result['time_seconds']}s")

    # üìú Gerar relat√≥rio
    print("\nüìä Resultado Final:")
    print("-" * 50)
    for r in results:
        print(f"üß† {r['model']}")
        print(f"‚è±Ô∏è Tempo: {r['time_seconds']}s")
        print(f"‚û°Ô∏è Resposta: {r['output']}")
        print("-" * 50)


if __name__ == "__main__":
    main()
